import requests
import telegram
import time
import asyncio
import os
import re
from dotenv import load_dotenv
from telegram.constants import ParseMode
from telegram.error import BadRequest
from playwright.async_api import async_playwright
import jieba
import jieba.analyse
from datetime import datetime, timezone, timedelta # å¯¼å…¥ç”¨äºå¤„ç†æ—¶é—´çš„æ¨¡å—

# --- é…ç½®åŠ è½½ (ä¿æŒä¸å˜) ---
load_dotenv()
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")
GNEWS_API_KEY = os.getenv("GNEWS_API_KEY")
if not all([TELEGRAM_BOT_TOKEN, TELEGRAM_CHAT_ID, GNEWS_API_KEY]):
    print("é”™è¯¯ï¼šé…ç½®ä¿¡æ¯æœªèƒ½å®Œå…¨åŠ è½½ã€‚")
    exit()

# --- ç­–ç•¥ä¸é…ç½® (ä¿æŒä¸å˜) ---
MAX_ARTICLES_TO_SEND = 3
SEND_INTERVAL_SECONDS = 20
UPDATE_INTERVAL_HOURS = 1
SENT_ARTICLES_FILE = 'sent_articles.txt'
SENT_TITLES_FILE = 'sent_titles.txt'
CHANNEL_TOPIC_HEADER = "ã€å…¨çƒæ–°é—»å¿«è®¯ã€‘"
CONTACT_LINK_TEXT = "è”ç³»æŠ•ç¨¿"
CONTACT_LINK_URL = "https://t.me/VIP33054"
GROUP_LINK_TEXT = "åŠ å…¥è®¨è®ºç¾¤"
GROUP_LINK_URL = "https://t.me/VIP31333"

# --- â˜… æ–°å¢ï¼šæ—¶é—´æ ¼å¼åŒ–å‡½æ•° â˜… ---
def format_china_time(time_str: str) -> str:
    """
    å°†å¤šç§æ ¼å¼çš„æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºâ€œå¹´-æœˆ-æ—¥ æ—¶:åˆ†â€çš„æ ¼å¼ã€‚
    """
    if not time_str:
        return "æœªçŸ¥"
    try:
        # æ›¿æ¢æ‰ 'Z' (UTC) æ ‡å¿—ï¼Œä»¥ä¾¿ fromisoformat å¤„ç†
        if time_str.endswith('Z'):
            time_str = time_str[:-1] + '+00:00'
        
        # ä½¿ç”¨ fromisoformat è§£ææ ‡å‡†æ—¶é—´æ ¼å¼
        dt_object = datetime.fromisoformat(time_str)
        
        # è½¬æ¢ä¸ºä¸­å›½æ—¶åŒº (UTC+8)
        china_tz = timezone(timedelta(hours=8))
        dt_object_china = dt_object.astimezone(china_tz)
        
        # æ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„ä¸­æ–‡æ ¼å¼
        return dt_object_china.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')
    except (ValueError, TypeError):
        # å¦‚æœè§£æå¤±è´¥ï¼Œåˆ™æŒ‰åŸæ ·è¿”å›ï¼Œç¡®ä¿ç¨‹åºä¸ä¼šä¸­æ–­
        return time_str.split('T')[0] # å°è¯•åªè¿”å›æ—¥æœŸéƒ¨åˆ†

# --- è¾…åŠ©ä¸æŠ“å–å‡½æ•° (ä¿æŒä¸å˜) ---
def load_sent_urls():
    if not os.path.exists(SENT_ARTICLES_FILE): return set()
    with open(SENT_ARTICLES_FILE, 'r', encoding='utf-8') as f: return set(line.strip() for line in f)
def save_sent_url(article_url):
    with open(SENT_ARTICLES_FILE, 'a', encoding='utf-8') as f: f.write(article_url + '\n')
def load_sent_titles():
    if not os.path.exists(SENT_TITLES_FILE): return set()
    with open(SENT_TITLES_FILE, 'r', encoding='utf-8') as f: return set(line.strip() for line in f)
def save_sent_title(article_title):
    with open(SENT_TITLES_FILE, 'a', encoding='utf-8') as f: f.write(article_title + '\n')
def get_gnews_news():
    print("æ­£åœ¨ä» GNews API è·å–æœ€æ–°æ–°é—»...")
    url = f"https://gnews.io/api/v4/top-headlines?lang=zh&country=cn&max=10&apikey={GNEWS_API_KEY}"
    try:
        response = requests.get(url, timeout=15)
        if response.status_code != 200: return []
        return response.json().get("articles", [])
    except Exception: return []
async def scrape_article_details(page, url: str) -> tuple[str, str]:
    pub_time, summary = "", ""
    try:
        await page.goto(url, timeout=30000, wait_until='domcontentloaded')
        time_selectors = ['meta[property="article:published_time"]','meta[name="publish-date"]','time','.pub_date','.post-time','.time-source .time']
        for selector in time_selectors:
            element = await page.query_selector(selector)
            if element:
                content = await element.get_attribute('content') or await element.get_attribute('datetime') or await element.inner_text()
                if content: pub_time = content.strip(); break
        content_selectors = ['article','.article-content','.post-body','.content','#article_content','#Content','.art-text','#main_content','div[class*="content-main"]','div[class*="article-body"]']
        for selector in content_selectors:
            content_element = await page.query_selector(selector)
            if content_element:
                paragraphs = await content_element.query_selector_all('p')
                summary_parts = [await p.inner_text() for p in paragraphs[:5] if await p.inner_text()]
                if summary_parts:
                    summary = "\n\n".join(summary_parts)
                    if len(paragraphs) > 5: summary += "..."
                    break
        return pub_time, summary
    except Exception: return pub_time, summary

# --- â˜…â˜…â˜… æ ¸å¿ƒæ”¹åŠ¨ï¼šå‘é€å‡½æ•°ï¼Œé‡‡ç”¨æ›´æ–°åçš„æ’ç‰ˆ â˜…â˜…â˜… ---
# --- â˜…â˜…â˜… æ ¸å¿ƒæ”¹åŠ¨ï¼šå‘é€å‡½æ•°ï¼Œå¢åŠ è¯¦ç»†é”™è¯¯æ—¥å¿— â˜…â˜…â˜… ---
async def send_single_article(bot, article, pub_time: str, summary: str):
    title, url, image_url = article.get('title'), article.get('url'), article.get('image')
    source_name = article.get('source', {}).get('name', 'æœªçŸ¥æ¥æº')
    if not title or not url: return False
    
    # (æ­¤éƒ¨åˆ†ä»£ç ä¿æŒä¸å˜...)
    display_time = format_china_time(pub_time) # å‡è®¾æ‚¨å·²ä½¿ç”¨æˆ‘ä¸Šæ¬¡æä¾›çš„å¸¦æ—¶é—´æ ¼å¼åŒ–çš„ç‰ˆæœ¬
    tags = jieba.analyse.extract_tags(title, topK=3)
    filtered_tags = [tag for tag in tags if not tag.isdigit()]
    hashtags = " ".join([f"#{tag}" for tag in filtered_tags]) if filtered_tags else ""
    summary_text = summary if summary else article.get('description', '')
    if summary_text and title in summary_text: 
        summary_text = ""
    if not summary_text:
        summary_text = f"å¦‚éœ€æ‘˜è¦ï¼Œè¯·<a href='{url}'>ç‚¹å‡»æ­¤å¤„</a>é˜…è§ˆã€‚"

    caption_parts = [
        f"{CHANNEL_TOPIC_HEADER} {hashtags}\n",
        f"<b>{title}</b>\n",
        summary_text,
        "", 
        f"è¯¦ç»†ä¿¡æ¯ï¼š<a href='{url}'>ç‚¹å‡»é˜…è¯»åŸæ–‡</a>",
        f"å‘å¸ƒæ—¶é—´ï¼š{display_time}",
        f"ä¿¡æ¯æ¥æºï¼š<a href='{url}'>{source_name}</a>",
        f"æŠ•ç¨¿è”ç³»ï¼š<a href='{CONTACT_LINK_URL}'>{CONTACT_LINK_TEXT}</a>",
        f"ğŸ’¬ æ¬¢è¿åŠ å…¥äº¤æµç¾¤è®¨è®ºï¼š<a href='{GROUP_LINK_URL}'>{GROUP_LINK_TEXT}</a>"
    ]
    caption = "\n".join(part for part in caption_parts if part.strip() or part == "")

    if len(caption) > 1024:
        oversize = len(caption) - 1024
        if "ç‚¹å‡»æ­¤å¤„" not in summary_text:
             summary_text = summary_text[:-(oversize + 5)] + "..."
             caption_parts[2] = summary_text
             caption = "\n".join(part for part in caption_parts if part.strip() or part == "")
        else:
             caption = caption[:1020] + "..."

    # â˜… å‘é€é€»è¾‘çš„å…³é”®æ”¹åŠ¨åœ¨è¿™é‡Œ â˜…
    try:
        if image_url:
            await bot.send_photo(chat_id=TELEGRAM_CHAT_ID, photo=image_url, caption=caption, parse_mode=ParseMode.HTML)
        else:
            await bot.send_message(chat_id=TELEGRAM_CHAT_ID, text=caption, parse_mode=ParseMode.HTML, disable_web_page_preview=True)
        return True
    except Exception as e:
        # æ‰“å°ç¬¬ä¸€æ¬¡å‘é€å¤±è´¥çš„è¯¦ç»†åŸå› 
        print(f"!!! å‘é€å¸¦å›¾ç‰‡çš„ç‰ˆæœ¬å¤±è´¥ï¼Œé”™è¯¯åŸå› : {e}")
        try:
            # å°è¯•ä»¥é™çº§æ¨¡å¼ï¼ˆä¸å¸¦å›¾ç‰‡ï¼‰å‘é€
            print("--- æ­£åœ¨å°è¯•å‘é€çº¯æ–‡æœ¬ç‰ˆæœ¬... ---")
            await bot.send_message(chat_id=TELEGRAM_CHAT_ID, text=caption, parse_mode=ParseMode.HTML, disable_web_page_preview=True)
            return True
        except Exception as fallback_e:
            # å¦‚æœé™çº§æ¨¡å¼ä¹Ÿå¤±è´¥äº†ï¼Œæ‰“å°æœ€ç»ˆçš„é”™è¯¯åŸå› 
            print(f"!!! å‘é€çº¯æ–‡æœ¬ç‰ˆæœ¬ä¹Ÿå¤±è´¥äº†ï¼Œæœ€ç»ˆé”™è¯¯åŸå› : {fallback_e}")
            return False

# --- â€œæ°¸åŠ¨æœºâ€ä¸»ç¨‹åº (ä¿æŒä¸å˜) ---
async def main():
    bot = telegram.Bot(token=TELEGRAM_BOT_TOKEN)
    print("GNewsã€ç»ˆæå®Œç¾æ’ç‰ˆç‰ˆã€‘æ–°é—»æœºå™¨äººæœåŠ¡å·²å¯åŠ¨ï¼")
    load_sent_articles = load_sent_urls
    save_sent_article = save_sent_url
    while True:
        try:
            print(f"\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] --- å¼€å§‹æ–°ä¸€è½®æ–°é—»æ£€æŸ¥ ---")
            sent_urls = load_sent_articles()
            sent_titles = load_sent_titles()
            news_articles = get_gnews_news()
            if not news_articles:
                print("æœªèƒ½ä»APIè·å–åˆ°æ–°é—»ã€‚")
            else:
                new_articles_found = [article for article in reversed(news_articles) if article.get('url') not in sent_urls and article.get('title') not in sent_titles]
                if not new_articles_found:
                    print("æ²¡æœ‰å‘ç°æ–°æ–°é—»ã€‚")
                else:
                    print(f"å‘ç° {len(new_articles_found)} æ¡æ–°æ–°é—»ï¼Œå‡†å¤‡å¤„ç†...")
                    async with async_playwright() as p:
                        browser = await p.chromium.launch(headless=True)
                        page = await browser.new_page()
                        articles_sent_count, sent_titles_this_run = 0, set()
                        for article in new_articles_found:
                            if articles_sent_count >= MAX_ARTICLES_TO_SEND:
                                print(f"æœ¬è½®å‘é€ä¸Šé™ ({MAX_ARTICLES_TO_SEND} æ¡)å·²åˆ°ã€‚")
                                break
                            current_title = article.get('title')
                            if current_title in sent_titles_this_run:
                                print(f"æ ‡é¢˜é‡å¤ï¼Œè·³è¿‡: {current_title}")
                                save_sent_article(article.get('url'))
                                continue
                            print(f"æ­£åœ¨å¤„ç†: {current_title}")
                            publication_time, summary = await scrape_article_details(page, article.get('url'))
                            if await send_single_article(bot, article, publication_time, summary):
                                save_sent_article(article.get('url'))
                                save_sent_title(article.get('title'))
                                sent_titles_this_run.add(article.get('title'))
                                articles_sent_count += 1
                                print(f"å‘é€æˆåŠŸ (æœ¬è½®å·²å‘é€ {articles_sent_count}/{MAX_ARTICLES_TO_SEND} æ¡)ã€‚")
                                if articles_sent_count < MAX_ARTICLES_TO_SEND and articles_sent_count < len(new_articles_found):
                                    await asyncio.sleep(SEND_INTERVAL_SECONDS)
                            else:
                                print(f"å‘é€å¤±è´¥: {current_title}")
                        await browser.close()
            
            sleep_seconds = UPDATE_INTERVAL_HOURS * 3600
            print(f"--- æœ¬è½®ä»»åŠ¡å®Œæˆï¼Œä¼‘çœ  {UPDATE_INTERVAL_HOURS} å°æ—¶... ---")
            await asyncio.sleep(sleep_seconds)
        except KeyboardInterrupt:
            print("\næœåŠ¡è¢«æ‰‹åŠ¨åœæ­¢ã€‚")
            break
        except Exception as e:
            print(f"!!! å‘ç”ŸæœªçŸ¥ä¸¥é‡é”™è¯¯: {e} !!!")
            print("å°†åœ¨ 15 åˆ†é’Ÿåé‡è¯•...")
            await asyncio.sleep(900)

if __name__ == '__main__':
    jieba.initialize()
    asyncio.run(main())